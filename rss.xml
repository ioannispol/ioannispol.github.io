<?xml version="1.0" encoding="utf-8"?>
<!--<?xml-stylesheet type="text/css" href="rss.css" ?>-->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
<title>Ioannis Polymenis</title>
<description>Updates from Ioannis Polymenis.</description>
<language>en-uk</language>
<link>http://ipolymenis.xyz/rss.xml</link>
<atom:link href="http://ipolymenis.xyz/rss.xml" rel="self" type="application/rss+xml" />

<!-- LB -->

<item>
<title>YOLO Object Detection for Underwater Images</title>
<guid>https://ipolymenis.xyz/2022.html#yolo-object-detection-for-underwater-images</guid>
<pubDate>Wed, 27 Apr 2022 11:23:02 +0100</pubDate>
<description><![CDATA[
<!-- directives:[] -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<div id="content">
    
    <h1 id="yolov1implementation">YOLO Object Detection for Underwater Images</h1>
    <p>YOLOv1 implementation based on the original paper <a href="[http://arxiv.org/abs/1506.02640](https://arxiv.org/pdf/1506.02640v5.pdf)">1</a></p>
    <h1 id="howyoloworks">How Yolo works</h1>
    <p>YOLO is an object detection algorithm and uses features that learned from a CNN network to detect objects. When performing object 
    detection we want to correctly identify in the image the objects in the given image. Most of the classic approaches in the object detection 
    algorithms using the sliding window method where the classifier is run over evenly spaces locations over the entire image. Such types of 
    algorithms are the Deformable Parts Models (DPM), the R-CNN which uses proposal methods to generate the bounding boxes in the given 
    image and then run the classifier on the proposed bounding boxes. This approach, and particularly the DPM method is slow and not optimal 
    for real time uses, and the improved version of R-CNN models is gaining some speed by strategically selecting interesting regions and run 
    through them the classifier.

</p>
    <p>On the other hand YOLO algorithm based on the idea to split the image in a grid, for example for a given image we can split it in a 3 by 3 
    grid (<strong><em>SxS = 3x3</em></strong>) which gives as 9 cells. As the below image shows, the image consists by a 3 by 3 grid with 9 cells, and each cell has 2 
    bounding boxes (<strong><em>B</em></strong>) which finally will give the prediction bounding boxes for the object in the image.</p>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/image.png><a href="../images/image.png"></a>
    </p>
    <p>Generally, the YOLO algorithm has the following steps:</p>
    <ol>
    <li>Divide the image into cells with an <strong><em>SxS</em></strong> grid</li>
    <li>Each cell predicts <strong><em>B</em></strong> bounding boxes (<em>A cell is responsible for detecting an object if the object's bounding box is within the cell</em></li>
    <li>Return bounding boxes above a given confidence threshold. <em>The algorithm will show only the bounding box with the highest probability confidence (e.g. 0.90) and will reject all boxes with less values than this threshold</em>.</li>
    </ol>
    <p><strong>Note:</strong> In practice will like those larger values of $S and B$, such as $S = 19$ and $B = 5$ to identify more objects, and each cell will output a prediction with a corresponding bounding box for a given image.</p>
    <p>The below image shows the YOLO algorithm's result, which returns the bounding boxes for the detected objects. For the algorithm to 
    perform efficiently needs to be trained sufficiently because with each iteration (epoch), the detection accuracy increases. Also, the 
    bounding boxes can be in more than one cells without any issue, and the detection is performed in the cell where the midpoint of the 
    bounding box belongs.</p>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/image2.png><a href="../images/image2.png"></a>
    </p>
    <p>The YOLO object detection algorithm is faster architecture because uses one Convolutional Neural Network (CNN) to run all components in 
    the given image in contrast with the naive sliding window approach where for each image the algorithm (DPM, R-CNN etc) needs to scan it 
    step by step to find the region of interest, the detected objects. The R-CNN for example needs classify around 2000 regions per image 
    which makes the algorithm very time consuming and it's not ideal for real time applications.</p>
    <p>The figure below shows how the YOLO model creates an $S x S$ grid in the input image and then for each grid cell creates multipe bounding 
    boxes as well as class probability map, and at the end gives the final predictions of the objects in the image.</p>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/yolo_paper.png><a href="../images/yolo_paper.png"></a>
    </p>
    <h2 id="howtheboundingboxesareencodedinyolo">How the bounding boxes are encoded in YOLO?</h2>
    <p>One of the most important aspects of this algorithm is the it builds and specifies the bounding boxes, and the other is the the Loss function. The algorithm uses five components to predict an output:</p>
    <ol>
    <li>The centre of a bounding box \(bx by\) relative to the bounds of the grid cell</li>
    <li>The width \(b_w\)</li>
    <li>The height \(b_h\). The width and the height of the entire image.</li>
    <li>The class of the object \(c\)</li>
    <li>The prediction confidence \(p_c\) which is the probability of the existence of an object within the bounding box.</li>
    </ol>
    <p>Thus, we, optimally, want one bounding box for each object in the given image and we can be sure that only one object will be predicted for each object by taking the midpoint of the cell that is responsible for outputting that object.</p>
    <p>So, each bounding box for each cell will have \(x1, y1, x2, y2\) coordinates where in the YOLO algorithm will be \(x, y, w, h\)</p>
    <ul>
    <li><p>\(x\) and \(y\) will be the coordinates for object midpoint in cell -&gt; these actually will be between \(0 - 1\)</p></li>
    <li><p>\(w\) and \(h\) will be the width and the height of that object relative to the cell -&gt; \(w\) can be <em>greater</em> than 1, if the object is wider than the cell, and \(h\) can also be <em>greater</em> than 1, if the object is taller than the cell</p></li>
    </ul>
    <p>The labels will look like the following:</p>
    <p>$$label_{cell} = [c_1, c_2, ..., c_5, p_c, x, y, w,h]$$</p>
    <p>where:</p>
    <ul>
    <li>\(c_1\) to \(c_5\) will be the dataset classes</li>
    <li>\(p_c\) probability that there is an object (1 or 0)</li>
    <li>\(x, y, w,h\) are the coordinates of the bounding boxes</li>
    </ul>
    <p>Predictions will look very similar, but will output two bounding boxes (will specialise to output different bounding boxes (wide vs tall).</p>
    <p>$$pred_{cell} = [c_1, c_2, ..., c_5, p_{c_1}, x_1, y_1, w_1, h_1, p_{c_2}, x_2, y_2, w_2, h_2]$$</p>
    <p><strong>Note:</strong> A cell can only detect one object, this is also one of the YOLO limitations (we can have finer grid to achieve multiple detection as mentioned above.</p>
    <p>This is for every cell and the <strong>target</strong> shape for one image will be \(S, S, 10\)
    where:</p>
    <ul>
    <li>\(S * S\) is the grid size</li>
    <li>\(5\) is for the class predictions, \(1\) is for the probability score, and \(4\) is for the bounding boxes</li>
    </ul>
    <p>The <strong>predictions</strong> shape will be \(S, S, 15\) where there is and additional probability score and four extra bounding box predictions.</p>
    <h2 id="themodelarchitecture">The model architecture</h2>
    <p>!model.png</p>
    <p>The original YOLO model consists of 24 convolutional layers followed by 2 fully connected layers.</p>
    <p>The model accepts 448x448 images and at the first layer has a 7x7 kernel with 64 output filters with stride of 2 (<strong>also need to have a 
    padding of 3 to much the dimensions</strong>), also there is a 2x2 Maxpool Layer with the stride of 2. Similarly, the rest of the model consists 
    of convolutional layers and Maxpool layers except the last two layers where there are a fully connected layers where the first one takes as 
    and input the convolutional output and make it a linear layer of 4096 feature vector and outputs to the fully connected which is reshaped 
    to become a 7 by 7 by 30 which is the final split size of the image \(S = 7\) which is a \(7 x 7\) grid with a vector output of 30 (in my case this 
    will be 15).</p>
    <p>To help with the architecture building it will be useful to pre-determine the architecture configuration:</p>
    <script src="https://gist.github.com/ioannispol/d9231587a6954e0bbc061fee326f5881.js"></script>
    <h2 id="thelossfunction">The Loss Function</h2>
    <p>The YOLO loss function is the second most important aspect of the algorithm. The basic concept behind all these losses is that are the 
    sum squared error, and if we look at the first part of the loss function is going to be the loss for the box coordinate for the midpoint (taking 
    the \(x\) midpoint value and subtractining from the predicted \(\hat{x}\) squared). The \(\mathbb{1}_{ij}^{obj}\) is the identity function which is calculated when there is an 
    object in the cell, so summurizing there is:</p>
    <ul>
    <li>\(\mathbb{1}_{i}^{obj}\) is 1 when there is an object in the cell \(i\) otherwise is 0.</li>
    <li>\(\mathbb{1}_{ij}^{obj}\) is the $j^{th}$ bounding box prediction for the cell \(i\)</li>
    <li>\(\mathbb{1}_{ij}^{noobj}\) has the same concept with the previous one, except that is 1 when there is no object and 0 when there is an object.</li>
    </ul>
    <p>So, to know which bounding box is responsible for outputing that bounding box is by looking at the cell and see which of the predicted 
    bounding boxes has the highest Intersection over Union (IoU) value with the target bouning box. The one with the highest IoU will be the 
    responsible bounding box for the prediction and will be send to the loss function.</p>
    <p>
        \begin{align}
            &\lambda_{coord} \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2 + (y_i-\hat{y}_i)^2 ] \\&+ \lambda_{coord} \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2 +(\sqrt{h_i}-\sqrt{\hat{h}_i})^2 ]\\
            &+ \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}(C_i - \hat{C}_i)^2 + \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{noobj}(C_i - \hat{C}_i)^2 \\
            &+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj}\sum_{c \in classes}(p_i(c) - \hat{p}_i(c))^2 \\
        \end{align}
    </p>
    <h2 id="algorithmimplementation">Algorithm Implementation</h2>
    <h3 id="yolomodelarchitecture">YOLO model architecture</h3>
    <h4 id="theyoloarchitecture">The YOLO Architecture</h4>
    <p>The CNNBlock class will be used as a block code to build the various convolutional layers in the YoloV1 class, which is the main model.</p>
    <script src="https://gist.github.com/ioannispol/0cd808100b3275a5f1367a3d983a5679.js"></script>
    <p>Then we test the YOLOv1 model as follows</p>
    <script src="https://gist.github.com/ioannispol/85524f4ffc02fafe1813ed16eb5f2df6.js"></script>
    <p>So, if we manually calculate the tensor shape we will get:</p>
    <p>$$
    S * S * (B * 5 + C)
    =&gt; 7 * 7 * (2 * 5 + 5) = 245 * 3 = 735
    $$
    <strong>Note:</strong> 3 is the number of channels in the photo (RGB)</p>
    <h3 id="codeimplementationofyololoss">Code implementation of Yolo loss</h3>
    <p>The implementation of the YOLO Loss based on the method described above.</p>
    <script src="https://gist.github.com/ioannispol/a70919c946e502fc8ad1b9dfab3ff9c4.js"></script>
	<div id="content"><h3 id="datasetclassforcustomdatasetfromlabs">Dataset Class for custom dataset from Labs</h3>
    <p>One of the most important aspects in Machine Learning and Deep Learning is to prepare the dataset that will be used from the model. There are different process that are used to load the data, pre-process them (apply transformations/augmentation) and import them to the model for training and testing. PyTorch gives us a really good method to simplify this process and allowing us to create custom data loaders and transforms  by writing Dataset classes.</p>
    <script src="https://gist.github.com/ioannispol/f5a96dab6a4626c64039f8a103d024a9.js"></script></div>
    
    <div id="content"><h3 id="trainingfunction">Training function</h3>
        <p>The Hyperpamaters used for the model training are as follows</p>
        <script src="https://gist.github.com/ioannispol/3eca5b9cf2d12ff7b9d62ec0ad9d2e4a.js"></script>
        <p>The training function includes the training loop which is used to loop over the dataset (epochs) and perform generally the following steps:</p>
        <ul>
        <li>zero the optimizer gradients at the beginning of the training</li>
        <li>execute the forward pass for the given training batch</li>
        <li>calculate the loss between the current state and the actual target</li>
        <li>use the calculated loss to perform the backward pass and update the weights on the model.</li>
        </ul>
        <script src="https://gist.github.com/ioannispol/ed603b966e71b261ce444b49e5a19a9f.js"></script>
        <p>The transformations were used is only the image resize to 448 by 488 pix to ensure that the image size much the model requirements.
            Finally, the main function will compile everything together, the model, the training and test datasets and loaders, to perform the training of the model.</p>
            <script src="https://gist.github.com/ioannispol/ec81dbe53728b203d9234d46cd12405d.js"></script>
        </div>

			<div id="content"><h3 id="results">Results</h3>
                <p>The YOLO v1 algorithm was modified to work correctly on the towing tank
                dataset. Thus, it needed to change the parameters for the input classes since the original model was
                trained on the Pascal VOC dataset containing 20 classes [3] while the towing tank dataset has only 5, and the dataset used for the object detection model was the initial towing tank dataset of 550 images.
                The results of the YOLO v1 model after 100 training epochs are shown in Figure 
                bellow, where shows the training and validation loss of the model. The training 
                and validation loss decreasing steadily through the training period as expected; 
                although the validation loss has a minor gap from the training loss, it decreases 
                almost at the same rate.</p></div>
        <p style="text-align:center">
            <img style="max-height:30em;max-width:30em;text-align: center" src=../images/yolo-loss.png><a href="../images/image.png"></a>
        </p>

        <p>Additionally, the gigure bellow shows the resulted images when the model tested on some test images.
            The model performs quite accurately even in the cases with low light conditions, as in Figures d
            and e. Similarly, it has satisfactory results for images with many different objects, such as Figures
            a, b, and c, and can identify objects with transparent texture as in the figure f. For this model,
            the objective was to train an object detection model and study its behaviour and its viability to using
            it further in the project; thus, only the basics were implemented, and details such as the name of each
            identified object in an image will be in the next iteration when using the YOLO v4 architecture.</p>
        <p style="text-align:center">
            <img style="max-height:30em;max-width:30em;text-align: center" src=../images/yolo-imgs.png><a href="../images/image.png"></a>
        </p>
        

    <hr />
    <h2 id="references">References</h2>
    <p>[1] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi,  You Only Look Once: Unified, Real-Time Object Detection&nbsp;, arXiv:1506.02640 [cs], May 2016, Accessed: Apr. 02, 2021. [Online]. Available: <a href="https://arxiv.org/pdf/1506.02640v5.pdf">http://arxiv.org/abs/1506.02640</a></p></div>
]]></description>
</item>


<item>
<title>Underwater Vehicles Technology Advances</title>
<guid>https://ipolymenis.xyz/2022.html#underwater-vehicles-technology-advances</guid>
<pubDate>Thu, 21 Apr 2022 16:12:29 +0100</pubDate>
<description><![CDATA[
<!-- directives:[] -->
			<div id="content"><h1 id="underwatervehiclestechnologyadvances">Underwater Vehicles Technology Advances</h1>
<hr />
<h2 id="introduction">Introduction</h2>
<p>Autonomous underwater robots have become an integral part of both marine science and ocean engineering, and with the 
evolution of these sectors, the development of AUVs is heading towards full autonomy. A complete autonomous underwater 
vehicle will operate independently and will be able to act logically and methodically to accomplish the required task. Over the 
past years, many attempts have been made to apply autonomy on AUVs in the form of surveillance and data collection, yet a 
fully autonomous vehicle that will be able to act intelligently within the operating environment has not been achieved (Ridao et 
al., 2015; Sahoo et al., 2019).</p>
<p>From the beginning of the 90s, various projects focused on different aspects of underwater autonomy. One of the first 
attempts was made with the autonomous vehicle ODIN, which was equipped with a one degree of freedom robotic arm and 
was capable of operating either autonomously through an onboard computer or via a tethered cable (Choi et al., 1994). 
Another attempt towards autonomous vehicles was the OTTER project conducted by MBARI’s research programme in 1995. 
The vehicle was equipped with a robotic arm, cameras for object detection and a variety of sensors including depth sensors, 
and compass. OTTER’s mission was to retrieve an object from the seabed and it was one of the first vehicles with computer 
vision, and interestingly for the first time, the idea of “intelligent control” for an underwater robot and the design of a complete 
autonomous vehicle was presented (Wang et al., 1995). Another significant project which developed in that period was the 
AMADEUS subsea robot. The robot was designed to handle and grasp underwater objects and was equipped with a pair of 
three-fingered hydraulic robotic arms. The double robotic arm was innovative for the time as there was cooperation between 
the two arms for object manipulation and with the use of the three-finger gripper, it would be possible to control the pressure 
when grasping an object, which is significant for precise gripping (Lane et al., 1997). Although different projects have been 
developed over the past years with the focus on creating a fully autonomous underwater vehicle, there has not yet been a 
vehicle which will intelligently interact within its operating environment. Therefore, the current project will develop a system 
capable of critically acting for underwater intervention and fill the gap of previous research.</p>
<h2 id="auvstateoftheart">AUV State-of-the-art</h2>
<p>During the last two decades, different projects started to develop, and the focus was to create an AUV capable of intervention. 
These projects include the EU-Thermie funded SWIMMER, the ALIVE project, the SAUVIM project, the TRIDENT I-AUV project, 
the GIRONA 500 and finally the MARIS project. The SWIMMER vehicle was an AUV which was capable of transporting a work-
class ROV to a dock station at the seabed. Once the AUV autonomously docked to the underwater platform, the ROV was 
deployed to execute the operation. The ROV was connected with a fibre optic cable to the SWIMMER AUV, which was deployed 
to perform its mission, to dock in a predefined underwater platform. With this configuration, there is no need for a support 
vessel with the necessary equipment to deploy and operate the ROV (Evans et al., 2001). Even though the earlier projects, 
SWIMMER, OTTER and AMADEUS, were pioneering for their era, they were still dependent on human supervision and guidance 
to perform the given task; this was revolutionised with the ALIVE project.</p>
<p>The ALIVE project was a breakthrough for underwater intervention missions due to its abilities in conducting physical 
manipulations while hovering to keep its position. The AUV was fitted with a seven degrees of freedom robotic arm and was 
capable of performing autonomous docking. The docking prcedure was performed based on the data received from sonars 
and cameras, which allowed the robot to estimate its position at any time. Once the docking had been achieved, the robotic 
arm was deployed to perform the preprogrammed task, which was the opening and closing of a valve in an underwater panel 
(Evans et al., 2003).</p>
<p>The advances in underwater robotics were continued with the SAUVIM project which was funded by the University of Hawaii. 
The Hawaiian project was conducted over the same period as the ALIVE project but followed a different approach for 
underwater intervention. The vehicle was designed to perform the manipulation task while hovering at the target area and to 
achieve the task, three different localisation approaches were used. Firstly, a sonar was used to perform long-distance 
searching to find the location of the desired target. Secondly, a different sonar was used for object identification to set the 
underwater robot to a specific position for manipulation. Finally, when the robot was at the manipulation position, an ultrasonic 
camera was used to perform the actual intervention. The robot was fitted with a seven degrees of freedom arm with a camera 
on its palm. During the tests, the robot performance was deemed satisfactory since it was able to identify and recover the 
target object (Marani et al., 2009).</p>
<p>In 2010 a new approach to underwater intervention was proposed with TRIDENT, an EU funded project. This project proposed 
flexible underwater operations using two different vehicles, one a surface vehicle and the second as the Intervention AUV (I-
AUV) which initially performs mapping of the area of interest then returns to transmit the data to the operator. Then, the 
operator decides on the desired task and the interventions, and the vehicle descends again to perform the instructed mission. 
The TRIDENT project was tested in an underwater environment and assessed for its cooperation capability between the 
vehicles and for intervention skills of the I-AUV to execute the recovery action (Ridao et al., 2015; Sanz et al., 2012).</p>
<p>The TRIDENT project was used to host another newly developed AUV capable of intervention, the GIRONA 500, which was 
developed by the Spanish University of Girona in 2012. The underwater vehicle was designed to operate in depths of up to 
500m and to be as reconfigurable as possible. The vehicle’s capacity to carry a variety of equipment and instruments 
depending on its mission was a distinct characteristic of the particular project. Also, the AUV was fitted with a range of 
thrusters where, in its most simple setup, it used three of them to control the vertical and the horizontal motion of the vehicle. 
In contrast, it could be fitted with eight thrusters which gave the vehicle the capability to operate in all six degrees of freedom 
for intervention tasks (Ribas et al., 2012).</p>
<p>GIRONA 500 is controlled using two computers, one is a low power computer which is used for the main tasks of the robot, 
and the second computer is more powerful and is dedicated for computational demand processes such as mapping and image 
processing. The second computer is operated only when it is needed; thus, power saving can be achieved. The software 
architecture implemented on the robot is based on three different operational layers which may exist independently. One layer 
is used for mission planning which can be done autonomously or by being given instructions through a language named 
Mission Controlled Language (MCL) and is executed under the vehicles software architecture, named Component 
Oriented Layer-Based Architecture for Autonomy (COLA2). The next layer is used to control the robotic arm, the execution layer, 
through cameras, sensors and actuators. The precise position of the arm can be set according to the 
target’s position using Petri Nets to perform the manipulation task. A full description of Petri Nets is given by (Palomeras et al., 2008). 
Lastly, the final layer is used to coordinate the mission and the execution layers. The coordination 
layer is referred to as the reactive layer and can execute a variety of tasks from the most simple such as starting a camera and 
different sensors, to the more advanced and complex such as 3-D navigation and object manipulation (Ribas 
et al., 2011).</p>
<p>The capabilities of GIRONA 500 were tested in an underwater environment where different tasks were given to the vehicle 
such as autonomous docking, valve manipulation and object recovery. Notably, the task of turning a valve was a milestone for 
underwater intervention since the robot had to perform the turning operation based on the Learning by Demonstration 
technique (LbD), also known as Programming by Demonstration (PbD) (Carrera et al., 2014). The LbD technique is based on 
the idea that the learning process can be achieved in an automatic programming method where the user presents or 
demonstrates, the new action or behaviour that the robot should learn rather than programming it through coding (Mohammad 
&amp; Nishida, 2015).</p>
<p>Finally, the Italian project MARIS developed with a focus on manipulation mainly for the offshore industry as well as for a 
variety of scientific tasks such as ocean surveying. The underwater robot uses a combination of sensors for navigation such 
as Doppler Velocity Loggers (DVL) and acoustic methods to be able to orient itself with reference to the seabed. The robot can 
detect objects using stereovision technology to generate a more accurate 3-D interpretation of the object. Moreover, it uses a 
sophisticated robotic arm which has been designed to intelligently perform grasping using an integrated system of force-
torque to “sense” the object. Another feature of the MARIS project is that the operation can be performed using one or more 
vehicles, where they can coordinate to execute the grasping of the object and then to transport it with safety for the object 
and the vehicle (Casalino et al., 2016).</p>
<h2 id="summary">Summary</h2>
<p>A variety of projects have been developed with the focus of underwater intervention. All projects have developed AUVs that 
are capable of autonomous data collection using a variety of onboard sensors, and also have the ability of physical 
intervention by performing pre-programmed tasks.</p>
<hr />
<h2 id="references">References</h2>
<ul>
<li>Casalino, G., Caccia, M., Caselli, S., Melchiorri, C., Antonelli, G., Caiti, A., Indiveri, G., Cannata, G., Simetti, E., Torelli, S., Sperindè, A., Wanderlingh, F., Muscolo, G., Bibuli, M., Bruzzone, G., Zereik, E., Odetti, A., Spirandelli, E., Ranieri, A., et al. (2016) Underwater intervention robotics: An outline of the Italian national project Maris. Marine Technology Society Journal. 50 (4), 98–107.</li>
<li>Carrera, A., Palomeras, N., Ribas, D., Kormushev, P. &amp; Carreras, M. (2014) ’An Intervention-AUV learns how to perform an underwater valve turning’, in Oceans 2014 - taipei. [Online]. November 2014 Institute of Electrical; Electronics Engineers Inc.</li>
<li>Choi, S.K., Takashige, G.Y. &amp; Yuh, J. (1994) ’Experimental study on an underwater robotic vehicle: ODIN’, in IEEE sympsium on autonomous underwater vehicle technology. [Online]. 1994 IEEE. pp. 79–84.</li>
<li>Evans, J.C., Keller, K.M., Smith, J.S., Marty, P. &amp; Rigaud, V. (2001) ’Docking techniques and evaluation trials of the SWIMMER AUV: An autonomous deployment AUV for workclass ROVs’, in Oceans conference record (ieee). [Online]. 2001 pp. 520–528.</li>
<li>Lane, D.M., O’Brien, D.J., Pickett, M., Davies, J.B.C., Robinson, G., Jones, D., Scott, E., Casalino, G., Bartolini, G., Cannata, G., Ferrara, A., Angelleti, D., Coccoli, M., Veruggio, G., Bono, R., Virgili, P., Canals, M., Pallas, R., Gracia, E., et al. (1997) AMADEUS: Advanced Manipulation for Deep Underwater Sampling. IEEE Robotics and Automation Magazine. 4 (4), 34–45.</li>
<li>Marani, G., Choi, S.K. &amp; Yuh, J. (2009) Underwater autonomous manipulation for intervention missions AUVs. Ocean Engineering. 36 (1), 15–23.</li>
<li>Mohammad, Y. &amp; Nishida, T. (2015) ’Learning from demonstration’, in Advanced information and knowledge processing. [Online]. pp. 293–317.</li>
<li>Ribas, D., Palomeras, N., Ridao, P., Carreras, M. &amp; Mallios, A. (2012) Girona 500 AUV: From survey to intervention. IEEE/ASME Transactions on Mechatronics. 17 (1), 46–53.</li>
<li>Ridao, P., Carreras, M., Ribas, D., Sanz, P.J. &amp; Oliver, G. (2015) Intervention AUVs: The next challenge. Annual Reviews in Control. 40227–241.</li>
<li>Sahoo, A., Dwivedy, S.K. &amp; Robi, P.S. (2019) Advancements in the field of autonomous underwater vehicle. Ocean Engineering. 181145–160.</li>
<li>Wang, H.H., Rock, S.M. &amp; Lee, M.J. (1995) ’Experiments in automatic retrieval of underwater objects with an AUV’, in Oceans conference record (ieee). [Online]. 1995 IEEE. pp. 366–373.</li>
</ul></div>
]]></description>
</item>


<item>
<title>Automate Anaconda Environment creation</title>
<guid>https://ipolymenis.xyz/2022.html#automate-anaconda-environment-creation</guid>
<pubDate>Mon, 18 Apr 2022 10:33:23 +0100</pubDate>
<description><![CDATA[
<!-- directives:[] -->
<div id="content"><h1 id="automateanacondaenvironmentcreation">Automate Anaconda  environment creation</h1>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/bash-automation.jpg><a href="../images/bash-automation.jpg"></a>
    </p>
    <h3 id="takingthemanualstepsandcreateautomation">Taking the manual steps and create automation</h3>
    <p>Creating a virtual environment in Anaconda is a straight forward task using the <code>conda create -n env-namme</code> command. By default, the new environment will be created in the folder of the anaconda/miniconda installation. For example in a Linux machine the environments are under the directory of <code>~/miniconda3/envs</code>. Now, if we are working in projects that involve creating multiple conda environments, it might become tedious to always creating this environments with the manual way, installing each and every package by sequentially type the commands on the terminal.  Of course we can add the most used packages to the <code>~/.condarc</code> file and save some time. But, the other day I had to create, <em>again</em>, a virtual environment and I thought that it might be useful to automate as much as possible I can the procedure of crating virtual environments. </p>
    <p><strong>Prerequisites</strong></p>
    <ul>
    <li>Linux </li>
    <li>For the script to work need the anaconda or miniconda to be installed (see my previous posts on how to install miniconda)</li>
    <li>If other than the bash shell is used (for example zsh), need to make sure the script will be properly executed. The easiest way is to just add <code>#!/usr/bin/env bash</code> at the top of the script so we can run it even if we use other shells</li>
    </ul>
    <h2 id="designingthecondaenvautomation">Designing the conda-env-automation</h2>
    <ol>
    <li>To create a virt env we use the <code>conda create -n test-env</code> command. Since the script would be asking the user for the name of the env, need to have a <em>user input</em> for the <em>name of  env</em>.</li>
    </ol>
    <pre><code class="bash language-bash">echo env_name
    </code></pre>
    <ol start="2">
    <li>Since, we use anaconda environments to work with different python versions and packages, need to have a input that determines the version of Python as well as the packages will be install during the environment creation.</li>
    </ol>
    <pre><code class="bash language-bash">conda create -n test-env python=3.x package1 package2
    </code></pre>
    <ol start="3">
    <li>Also can ask to activate the newly created environment</li>
    </ol>
    <pre><code class="bash language-bash">conda activate test-env
    </code></pre>
    <h3 id="optionaloperations">Optional operations</h3>
    <ol>
    <li>Create an export (backup file) of the environment. This is helpful if someone else want to reproduce our work using the same environment.</li>
    </ol>
    <pre><code class="bash language-bash">conda test_env export &gt; environment_backup.yml
    </code></pre>
    <h4 id="adiagramrepresentationofthesteps">A diagram representation of the steps</h4>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/conda-env-algorithm.png><a href="../images/conda-env-algorithm.png"></a>
    </p>
    <h4 id="codeimplementation">Code Implementation</h4>
    <p>Initially will set a variable name for the location of the environments directory in the miniconda location. Also, will give the list of any existing environments. </p>
    <pre><code class="bash language-bash">ENV_PATH="$HOME/miniconda3/envs"
    echo "The folowing environments are installed:$(ls $ENV_PATH)"
    </code></pre>
    <p>Then, will check if the environment we like to crate exists and if so the process will exit the script</p>
    <pre><code class="bash language-bash">dir_path="$ENV_PATH/$env_name"
    
    
    if [ -d "$dir_path" ];
    
    then
    
        echo "$env_name environment exists."
    
        slee 1
    
        exit 0
    
    else
    
        echo "$env_name environment does not exist."
    
        sleep 1
    
        echo "Will create the $env_name conda environment"
    
    fi
    </code></pre>
    <p>The <code>dir_path</code> variable gives the location of the environment, if that exists in the <code>envs</code> folder, The <code>if</code> statement looks for the particular environment, and if that exists will exit the script, if not the will continue to create the environment. </p>
    <p>The conda environments are used in the most of the cases to have a specific python version. So, the next part will ask for what python version of python we want. </p>
    <pre><code class="bash language-bash">read -rp "Enter the Python version(only number): " py_ver
    </code></pre>
    <p>Also, most of the times we install particular packages will we create the environment. This can be done with following</p>
    <pre><code class="bash language-bash">read -rp "Enter additional packages to install: " packages
    </code></pre>
    <p>Now that we have the python version and the packages we can create the environment</p>
    <pre><code class="bash language-bash">conda create -n $env_name python=$py_ver $packages -y
    </code></pre>
    <p>The environment now has been crated and can use it by activating it with the <code>conda activate test-env</code> for example. 
    We can further automate the process and prompt a question if we want to use the env after the creation. The script will ask if we want to activate the environment, if yes specified then will enter to it and if no will continue to the next step, witch is to create an export of the environment. </p>
    <pre><code class="bash language-bash">read -rp "Do you like to activate $env_name environemnt?" ask &amp;&amp;
    
        if [ "$ask" = "y" ]; then
    
            printf "Activating the environment...\\n"
    
            conda activate $env_name
    
        else
    
            continue
    
        fi
    </code></pre>
    <p>Sometimes we might need to use a specific environment to another computer, or to send it to colleagues in our team for example to replicate the work we've done. This can be done if we export the <code>.yml</code> of the environment. So, the process can be automated to export the basic environment created initially. The script will ask if we want a backup file, if no the script will exit. If we do want to create a backup file of the environment then need to activate it and export the environment. Last, will deactivate the environment and exit the script. </p>
    <pre><code class="bash language-bash">read -rp "Do you want to create a backup for the $env_name?" ask &amp;&amp;
    
        if [ "$ask" = "y" ]; then
    
            printf "Creating the backup.yml\n"
    
            conda activate $env_name &amp;&amp; conda env export &gt; "$env_name.yml"
    
            conda deactivate
    
            echo "The backup file crated."
    
        else
    
            exit 0
    
        fi
    </code></pre></div>
]]></description>
</item>


<item>
<title>3yrs into Machine Learning as a PhD student</title>
<guid>https://ipolymenis.xyz/2022.html#3yrs-into-machine-learning-as-a-phd-student</guid>
<pubDate>Thu, 14 Apr 2022 17:16:21 +0100</pubDate>
<description><![CDATA[
<!-- directives:[] -->
<div id="content"><h1 id="3yrsintomachinelearningasaphdstudent">3yrs into Machine Learning as a PhD student</h1>
    <p style="text-align:center">
        <img style="max-height:30em;max-width:30em;text-align: center" src=../images/patrick-tomasso-Oaqk7qqNh_c-unsplash.jpg><a href="../images/istockphoto-1328881695-612x612.jpg"></a>
    </p>
    <h2 id="introduction">Introduction</h2>
    <p>So it has been almost 3 years and a half since I started my PhD in Autonomous Underwater Vehicles (AUVs), where I'm using Machine Learning, Deep Learning and Computer Vision to create a framework for autonomous underwater intervention. Puff, this was relatively quick to describe 3 and 1/2 years in one sentence. Anyway, to the point. So, what I've learned during those years, and how easy (or maybe not) it's to get into the magic world of AI? Well, I realised that it's not easy at all. Getting into ML is not an easy task and needs to have a lot of patience. </p>
    <p>Learning the concepts of Machine Learning, Computer Vision, and Deep Learning is something that initially I thought could be learnt in a year or two. After all, I knew most of the mathematical concepts needed for ML since I was in a STEM field for the past four years. I had relatively advanced knowledge of Algebra, Matrix Algebra, I was reasonably fluent in calculus, etc. I was good at problem-solving through my years in university, but a professional marine engineer for more than 10 years. </p>
    <p>I learned those 2 years that there is no such thing as "I know Machine Learning". This is because fields such as Machine Learning rapidly evolve, particularly in the last years, making it even harder to learn the entire area (if this is possible) even after many years in the field. </p>
    <p>So, in this article, I would like to give the experience I had over the past 3 years, mainly studying Machine Learning for my PhD research project. This point of view will definitely be biased towards the more academic point of view since I lack the experience that someone has working in the industry.    </p>
    <p>First, I'll go through the road I followed from the beginning until now. Then I'll spend some time on what I did right and wrong. Last, I want to give some of my thoughts on whether the journey worth or not.</p>
    <hr />
    <h2 id="myroadmaptoml">My roadmap to ML</h2>
    <p>We leave indeed in extraordinary times in terms of information and availability of learning material for almost anything we want to learn. Of course with so many options ready available, and most of the time free, to us it's difficult to choose and get the work done. And the vast variety of material,  training videos and courses was the enemy for me, because I was so exited about learning so many interesting things about Machine Learning, Deep Learning, Computer Vision, about the advances in Autonomous Underwater Vehicles (AUVs) and many more, were I realised that I'm not productive at all learning concepts that were irrelevant to my cause. </p>
    <p>Realising the problem was a catalyst to sit down and plan what I needed, at the time, to learn and at what concepts I should be focusing on. So, my Machine Learning study plan included the following:</p>
    <ul>
    <li>Get started with programming (Python)</li>
    <li>Find the relevant material need for Machine Leaning<ul>
    <li>Mathematics</li>
    <li>Statistics</li>
    <li>Linear Algebra</li>
    <li>Calculus</li>
    <li>Algorithms</li></ul></li>
    <li>Take online classes <ul>
    <li>MIT OpenCourseWare</li>
    <li>Coursera </li>
    <li>Udacity</li></ul></li>
    <li>Projects<ul>
    <li>Image classifier</li>
    <li>Object detection</li>
    <li>GANs</li></ul></li>
    <li>Application of what I learned in my project<ul>
    <li>Underwater image classifier</li>
    <li>Underwater object detection</li>
    <li>Underwater cycleGAN image dataset </li></ul></li>
    </ul>
    <h4 id="programmingandpython">Programming and Python</h4>
    <p>Programming for me was on top of my priorities because I had no previous xperience, though I did some assignments during my Bachelor’s and Master’s degrees using mainly Matlab and to some extent Python.&nbsp;<br />
    Of course for someone that starts to learn ML the language of choice, in most cases is Python. I don’t like to stress how I learned Python or what resources I used here (I might go my experience learning Python in another writing), rather give a quick overview. So, I started to learn Python by doing what everybody else does, watching youtube videos. Soon I realised that watching videos and doing tutorials won't help me to get into programming. So I started to learn the basics by developing small programs that ultimately will be useful to my PhD project. Such programs helped me to automate folder and file manipulation, image pre-processing, object extraction from annotated images, web scraping to download underwater images and videos, frame extraction from underwater videos etc. The above "<em>side projects</em>" wasn't the most efficient or the most elegant in terms of construction, but helped me not to fall into the loop of spending my time watching videos, pretend that I understand what I was watching and at the end not know to how to loop over a list. Programming my own staff helped me to understand that:</p>
    <ul>
    <li><strong>Understand why to program</strong> - Programming is not about the language we use, rather how we use the language to do some tasks (particularly the boring ones) for us</li>
    <li><strong>What area we are interested</strong> - It would be useless if we spend countless ours to learn let say C or C++ and at the end we would like to be a front-end web developers (even though I tried C++ and helped me a lot to better understand how the program works, and most importantly to comprehend programming principles).</li>
    <li><strong>One equals to 0</strong> - Definitely need to spend time to learn at least a second language once feel confidence with the first language. </li>
    <li><strong>Practice makes perfect</strong> - Let me be honest, at the beginning I was overwhelmed by the amount of what I needed to learn to create even the simplest program. It was so difficult for me because I was lost in the countless videos and tutorials, and I was thinking that OK I'm not good at programming lets just quit. Fortunately, I'm a very stubborn person so I kept going. Then, I noticed that I just needed to apply my problem solving skills had acquired so far, brake the problem into small manageable pieces and use simple language features (Python) to solve that problem. After that I needed to keep go over and over again, make practice and try to improve even a tiny aspect of the program I wrote. At the end, whit practice and patience I could easily write scripts for my needs.     </li>
    </ul>
    <h4 id="machinelearningonlinematerial">Machine Learning online material</h4>
    <p>For maths I wanted to get up to date as quickly as possible, since I already knew most of the material I needed, and for this reason I mostly used online materials. Some excellent resources include: 
    <strong>Linear Algebra and Calculus</strong>
        - <a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/">Video Lectures | Linear Algebra | Mathematics | MIT OpenCourseWare</a> 
        - <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of linear algebra - YouTube</a>
        - <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Essence of calculus - YouTube</a>
    <strong>Statistical Machine Learning</strong>
        - <a href="https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/video_galleries/lecture-videos/">Lecture Videos | Statistics for Applications | Mathematics | MIT OpenCourseWare</a>
        - <a href="https://www.youtube.com/watch?v=tcusIOfI_GM">Statistics for Data Science & Machine Learning - YouTube</a>
        - <a href="https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC">Statistical Machine Learning — Ulrike von Luxburg, 2020 - YouTube</a></p>
    <p>Additionally to the online materials I used some math text books that I found interested to study:</p>
    <ul>
    <li><a href="https://mml-book.github.io/">Mathematics for Machine Learning</a> The Book covers the foundations need to use in ML as well as the use of different ml algorithms that using the theoretical concepts.</li>
    </ul>
    <h4 id="onlinecourses">Online Courses</h4>
    <ul>
    <li><a href="https://ocw.mit.edu/courses/6-867-machine-learning-fall-2006/">Machine Learning MIT OpenCourseWare</a></li>
    <li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning by Stanford University | Coursera</a></li>
    <li><a href="https://www.udacity.com/course/machine-learning-engineer-for-microsoft-azure-nanodegree--nd00333">Become a Machine Learning Engineer</a></li>
    <li><a href="http://introtodeeplearning.com/">MIT Deep Learning 6.S191</a></li>
    <li><a href="https://www.coursera.org/specializations/deep-learning">Deep Learning by deeplearning.ai | Coursera</a></li>
    </ul>
    <h4 id="projects">Projects</h4>
    <p>I started to do the projects that the courses provided, and then once I was confident enough I applied what I knew to my own project.</p>
    <ul>
    <li><a href="https://ipolymenis.xyz/cat-vs-dog-img-classifier.html">CAT-VS-DOG IMAGE CLASSIFIER</a></li>
    <li>Object detection using YOLO algorithm</li>
    <li><a href="https://github.com/ioannispol/Udacity_Machine_Learning_ND/tree/main/Capstone_Project/dog-breed-classifier-project">Dog-breed-classifier-project</a></li>
    <li>Image-to-Image translation using cycleGANs</li>
    </ul>
    <h4 id="applicationtomyproject">Application to my project</h4>
    <ul>
    <li><a href="https://github.com/ioannispol/underwater_classifier">underwater_classifier</a></li>
    <li>YOLO underwater object detection</li>
    <li><a href="https://github.com/ioannispol/UnderWaterGAN">UnderWaterGAN: CycleGAN model to generate images with underwater features</a></li>
    </ul>
    <h2 id="whatididright">What I did right</h2>
    <p>Sometimes I think that I could have spent my time and energy more efficient when studying Machine Learning. But in a broader view, I did learn the concepts in the correct order. I started with the mathematics required to get confident when using ML, and at the same time studied Python and Programming, then started to study machine learning, next followed Deep Learning and Computer vision. Finally, I used the concepts to work on my PhD project. 
    To give the above studying approach on a time scale, the time I devote to each of these concepts is as follows:</p>
    <ol>
    <li><strong>Mathematics</strong>  - The first step was to revise mathematics that I already knew and study the concepts that I was less familiar with, such as statistics. So, I spend a good month revising Linear Algebra, Matrix Algebra and calculus, and another month studying statistics. (<em>2 months</em>)</li>
    <li><strong>Programming (Python)</strong> - I would be lying if I said that I have studied programming and particularly <em>Python</em> for the past 3 years, but I don't exaggerate about that. Ok, I didn't spend the last 3 years solely studying programming and Python, but I definitely constantly practising and trying to develop my skills (<em>3 years</em>).  </li>
    <li><strong>Machine Learning</strong> - Again, same with the programming I am constantly study and researching about ML, DL and CV. The field is so rapidly evolving that need to be alert about the advances happening. It's quite difficult to keep up but fortunately there is a solution for that. Some time ago I came across with an amazing tool, the <a href="https://researchrabbitapp.com/">Research rabbit</a> platform and it saved my day. Research rabbit is an online platform that can sync the research tools such as Zootero and Mendelay, and visually represent the papers connected to the paper we are working with, and all the papers that referencing the one we are working with.   </li>
    </ol>
    <h2 id="whatididwrong">What I did wrong</h2>
    <p>When you start to learn something new will never think that the way you do the preparation, the research and in general the path you follow might be a wrong. And even if you do know that might need to do things in a different way, at the time you will realize it, you have already spent a good amount of time. So, for me there isn't something that I wish I had do it differently, except the fact that in some cases I spent much more time that really needed in topics that wasn't so important, or at least not important to my project anyway. For example, I remember spending a fair amount of time revising Linear Algebra (LA) and Matrices when I started with ML. This of course isn't so bad, but I think that I could use that time more wisely, so to spend 1/3 of the time in LA revision and the rest of the time to implement that knowledge in Python.    </p>
    <h2 id="finalthoughts">Final thoughts</h2>
    <p>As my PhD studies are inevitably approaching the end I had the urge to go through where I was and what I've done so far. This writing is more like a clearing my thoughts about the journey I decided to take almost 3 years ago, and I realised that I enjoy to learn <em>Programming</em>, research about <em>Machine Learning</em>, and do <em>Research</em>.     </p></div>
]]></description>
</item>


<item>
<title>How to prepare for an MSc. How is it different to BSc?</title>
<guid>https://ipolymenis.xyz/2022.html#how-to-prepare-for-an-msc-how-is-it-different-to-bsc</guid>
<pubDate>Wed, 13 Apr 2022 17:10:07 +0100</pubDate>
<description><![CDATA[
<h2>How to prepare for an MSc. How is it different to BSc?</h2>
<p><strong>A guide to  prospective students want to do an MSc at Newcastle University</strong></p>
<p style="text-align:center">
    <img style="max-height:30em;max-width:30em;text-align: center" src=../images/ncl-tnk.jpg><a href="../images/istockphoto-1328881695-612x612.jpg"></a>
</p>

<p>The MSc is the standard Masters qualification for <strong>taught</strong> courses in <strong>Science and Technology</strong> subjects such as Marine Engineering and Naval Architecture. It stands for <em>Magister Scientiae</em>, which is Latin for ‘Master of Science’. In the <strong>UK</strong>, an MSc is usually a one-year full-time course amd unlike an undergraduate degree (BSc), the teaching and assessment period runs for a full 12 months. You will have a summer holiday during your degree, but this is when you’ll be expected to complete your dissertation (no one said postgraduate study would be easy!). In order to qualify as a full Masters programme, an MSc will conclude with an extended project and written dissertation which consist the majority of the required credits. You’ll be assigned a supervisor, but will be independently responsible for researching your topic, assembling data and analysing your findings. Whereas in a BSc course the majority of the credits are obtained through the different modules. This will be your chance to apply the expertise you’ve acquired on your Masters whilst pursuing your own specialist interests. The experience will be a great preparation for professional research work, or even for a PhD.</p>

<p>To prepare for an MSc in Marine Technology have the following in mind: 
<ol>
<li>Check the <a href="https://www.ncl.ac.uk/engineering/study-with-us/postgraduate-taught/marine-offshore-engineering/">University’s website</a>. This is the best way to start preparing for your upcoming studies since it has many information about the course and gives the best idea what to expect during the studies. Also, there is detailed information about the modules, how they assessed and give directions to other useful resources.</li>
<li>Explore the career opportunities. Again, staring from the university’s official web page, explore the paths available for the chosen course and look for current opportunities in that area using sites like Linkdin.</li>
<li>Language requirements. If English is not your mother tongue take some time to spot any gaps that might affect your progress during the course. Since, Marine Engineering and Naval Architecture are highly technical subjects the best recommendation is to immerse yourself in the basic scientific and engineering terminology even before you start the course.</li>
<li>Get in touch. Though many information study material and other resources can be found online some times its intimidating to go through everything. Thus, the best option is to ASK for help. Getting in touch with someone for the university might be the best option when you feeling lost or need to ask something more specific. The university offers a variety of options through emails to chatting platforms like <a href="https://www.ncl.ac.uk/study/contact/unibuddy/">Unibuddy</a>. </li>
<li>Stay CALM. Easy to say than doing it :). But the trough is that a key ingredient to success in your Postgraduate studies is to be RELAXED from the very beginning. No one is expecting from you to be perfect and know everything from the first day. You will have enough time and opportunities to develop the necessary skills need for your chosen subject. And the best of all is that the academic staff is on your side, they want you to succeed as much as you want to have a successful year.</li>
</ol>
</p>
]]></description>
</item>


<item>
<title>Virtual Environments
 #Wed, 13 Apr 2022 16:26:14 +0100</title>
<guid>https://ipolymenis.xyz/2022.html#virtual-environments</guid>
<pubDate>Wed, 13 Apr 2022 16:26:14 +0100</pubDate>
<description><![CDATA[
<h2>Virtual environments</h2>
            <p style="text-align:center">
                <img style="max-height:30em;max-width:30em;text-align: center" src=../images/blog-img.jpeg><a href="../images/blog-img.jpeg"></a>
            </p>
    <p>Developing Machine Learning (ML) and Deep Learning (DL) projects have become more common.
        ML and DL are literary everywhere nowadays, from big corporations to schools and universities, 
        and used in areas from medicine to robotics. But with the extensive use and variability of the 
        projects where ML/DL is used, some problems need to tackle. When working on and developing such 
        projects, the most common issue is the conflict between different libraries. This problem can 
        easily be solved if we use virtual environments.</p>

    <p>A virtual environment is precisely what the word describes, an environment, a sandbox, where we 
        install libraries and software particular to the project we're working on. In data science, ML and DL, 
        the Anaconda environments are the most well-known platforms used to create and manage virtual environments.</p>

    <p>Anaconda environments offer a simple and elegant solution to manage our settings based on the project requirements we're working on. 
        I think the power of anaconda environments comes from using a different version of python from a single installation. 
        Also, we can easily use other anaconda repositories to install the GPU versions of ML/DL frameworks such as TensorFlow and PyTorch with ease.</p>

    <h3 id="installation">Installation</h3>

    <p>The miniconda, a minimal version of Anaconda, will be installed here. Download the latest miniconda 
        Linux installer script at https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. 
        Also, at <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda — Conda documentation</a>, 
        different miniconda installers and for other platforms (Windows, MacOSX, Linux).</p>

    <p>Once the installer is downloaded, open a terminal and navigate to the location the script has been downloaded. 
        For me is in <code>/home/myusername/Downloads</code> First, check and verify the integrity of the installer</p>

    <div class="sourceCode" id="cb1"><pre class="sourceCode zsh"><code class="sourceCode zsh"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>sha256sum Miniconda3-latest-Linux-x86_64.sh</span></code></pre></div>
    <p>and will get something similar to <code>4ee9c3aa53329cd7a63b49877c0babb49b19b7e5af29807b793a76bdb1d362b4</code> 
        this value needs to compare it with the one given on the miniconda site. If the values much mean the download is 
        not corrupted and can continue with the installation.</p>

    <p>Then need to make the script executable (if it is not already) using the <code>chmod</code> command and then run the installer itself with the 
        <code>./program-to-install</code> This can be done by running the following command</p>

    <div class="sourceCode" id="cb2"><pre class="sourceCode zsh"><code class="sourceCode zsh"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">chmod</span> +x Miniconda3-latest-Linux-x86_64.sh <span class="kw">&amp;&amp;</span> ./Miniconda3-latest-Linux-x86_64.sh</span></code></pre></div>
    <p>The installation procedure is pretty clear after that point. Need to scroll down to the end of EULA terms and conditions and type 
        <code>yes</code> to agree with them.</p>

    <p>Also, it is a good idea to agree with the default installation path, which is usually under the path 
        <code>/home/myusername/miniconda3</code>. The installation directory can also be specified to a different location, 
        but you need to add it to the path variable on your system to run miniconda.</p>

    <p>The following prompt is essential and will ask to initialize miniconda after the installation and need to type <code>yes</code>. 
        If following the default, <code>no</code> will need to initiaize miniconda manually. To do so need to manually initialize 
        the shell you are using. For example, my shell is <code>zsh</code>, so the command will be <code>conda init zsh</code> 
        and then need to restart your shell with</p>

    <div class="sourceCode" id="cb3"><pre class="sourceCode zsh"><code class="sourceCode zsh"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">source</span> ~/.zshrc</span></code></pre></div>
    <p>Now, the last step is not necessary, but Anaconda activates the base environment whenever you open your terminal by default, 
        and I found that a bit annoying. So to remove the default functionality need to execute the following</p>
]]></description>
</item>


<item>
<title>Virtual Environments with GPU support
 </title>
<guid>https://ipolymenis.xyz/2022.html#virtual-environments-with-gpu-support</guid>
<pubDate>Wed, 13 Apr 2022 16:27:59 +0100</pubDate>
<description><![CDATA[
<h2>Anaconda configuration for CPU and GPU</h2>
                <p>Deploying a Machine Learning and Deep Learning project is not always straight forward particularly 
                    when we need to run the model in the GPU if there is one in our machine. First of all, we need 
                    to have a GPU that supports the CUDA library. The list of the CUDA supported GPUs can be found <a href="https://developer.nvidia.com/cuda-GPUs">here</a>. 
                    The next step is to specify which framework we intend to use for our deployment, it will be TensorFlow, Pytorch, MXNET etc, 
                    for this example, we will use the TensorFlow.</p>
                <p>Before installing anything it is the best practice to use virtual environments to install the specific versions that we 
                    want and keep the main system intact, otherwise by installing different versions for different projects we will face 
                    many difficulties with the conflicting libraries. Here we'll use Anaconda to set up our virtual environments.</p>
                <h2 id="installing-anaconda">Installing Anaconda</h2>
                <p>The cool thing about using Anaconda is that allows us to install the required from the GPU drivers and not bother 
                    to install the drivers locally to the machine, which could be rather a messy procedure. The other best thing when 
                    using Anaconda environments is that we can have different environments with different CUDA and TensorFlow versions. 
                    Also, when installing Anaconda Python is included.</p>
                <p>I am using the Miniconda version which is much lighter than the full Anaconda version, and the Linux version can be downloaded from<a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh">here</a>.
                    After the installation has finished we need to exit the terminal and open it again in order for the installation of Anaconda to take effect.</p>
                <p>Before we setup the environment need to find the correct versions of CUDA, cuDNN, etc required by <a href="https://www.tensorflow.org/install/gpu">TensorFlow</a>.</p>
                <h2 id="tensorflow-requirements">TensorFlow Requirements</h2>
                <p>To istall <strong>TensorFlow 2.4</strong> and <strong>TensorFlow 2.4 GPU</strong>
                    we need: 
                    <ol>
                    <li>Python 3.5 - 3.8 </li>
                    <li>For Linux, Ubuntu 16.04 or later (we can use
                        other Ubuntu based distros as well) </li>
                    <li><a href="https://www.nvidia.com/drivers">NVIDIA® GPU drivers</a> —CUDA®
                        11.0 requires 450.x or higher. </li>
                    <li><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA® Toolkit</a>
                        —TensorFlow supports CUDA® 11 (TensorFlow &gt;= 2.4.0) </li>
                    <li><a href="http://docs.nvidia.com/cuda/cupti/">CUPTI</a> ships with the CUDA®
                        Toolkit.</li> 
                    <li><a href="https://developer.nvidia.com/cudnn">cuDNN SDK 8.0.4</a>
                        <a href="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN versions</a>.</li>
                    <li>7. (Optional) <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html">TensorRT
                        6.0</a> to improve latency and throughput for inference on some models.</li>
                    </ol>
                </p>
            </h2>

            <h2>Search Anaconda repos for the needed packages</h2>
            <p>Since we'll use conda environments to install all the necessary drivers, only we need to 
                ensure that the machine has the correct Nvidia GPU drivers (450.x or higher). 
                Then we need to make check the Anaconda repositories to find if the above drivers exist. 
                Fortunately, we can make use of the conda-forge channel, as well as th Nvidia channel to install these drivers. 
                We can do that as follows:</p>

            <pre style=max-width:600px;margin:auto;>
                <code>conda search cuda</code>
                <code>Loading channels: done
                    No match found for: cuda. Search: *cuda*
                    # Name                       Version           Build  Channel             
                    cudatoolkit                      9.0      h13b8566_0  pkgs/main           
                    cudatoolkit                      9.2               0  pkgs/main           
                    cudatoolkit                 10.0.130               0  pkgs/main           
                    cudatoolkit                 10.1.168               0  pkgs/main           
                    cudatoolkit                 10.1.243      h6bb024c_0  pkgs/main           
                    cudatoolkit                  10.2.89      hfd86e86_0  pkgs/main           
                    cudatoolkit                  10.2.89      hfd86e86_1  pkgs/main           
                    cudatoolkit                 11.0.221      h6bb024c_0  pkgs/main</code>
            </pre>

            <p>For cudnn will use the Nvidia channel:</p>

            <pre style=max-width:600px;margin:auto;>
                <code>conda search -c nvidia cudnn</code>
                <code>Loading channels: done
                    # Name                       Version           Build  Channel                       
                    cudnn                          8.0.0      cuda10.2_0  nvidia              
                    cudnn                          8.0.0      cuda11.0_0  nvidia              
                    cudnn                          8.0.4      cuda10.1_0  nvidia              
                    cudnn                          8.0.4      cuda10.2_0  nvidia              
                    cudnn                          8.0.4      cuda11.0_0  nvidia              
                    cudnn                          8.0.4      cuda11.1_0  nvidia</code>
            </pre>

            <p>So, for TF-2.4 we'll install:
                <ol>
                    <li>Python 3.8</li>
                    <li>cudatoolkit 11.0</li>
                    <li>cudnn 8.0.4</li>
                    <li>tensorflow-gpu 2.4</li>
                </ol>
                Now that we have found the correct versions of the necessary libraries and drivers for the TF-2.4, 
                the next step is to create the virtual environment, as the official Anaconda 
                <a href="https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/">documentation</a>
                describes, that will host the packages and libraries required by TensorFlow with the following command:
            </p>

            <pre style=max-width:600px;margin:auto;><code>conda create -n tf24-cuda11 python=3.8</code></pre>
            <p>After creating the <code>tf24-cuda11</code> environment, environment, we install the above packages plus the jupyter lab:</p>

            <pre style=max-width:600px;margin:auto;><code>conda install cudatoolkit</code>
                
<code>conda install -c nvidia cudnn=8</code>
                    
<code>pip install tensorflow-gpu</code>
            </pre>

            <p>The <code>pip install tensorflow-gpu</code> command will install the Tensorflow GPU version, 
                Tensorflow estimator, and Tensorflow base. We don't use conda to install TensorFlow-GPU because 
                the latest TensorFlow version in the conda repo is 2.2 for Linux and 2.3 for Windows. 
                If we do use<code>conda install tnsorflow-gpu</code>, it will install also CUDA 10.2 and cuDNN 7.</p>
            <p>The <a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">Jupyter
                    Lab</a> can be installed from the conda-forge channel:</p>

           <pre style=max-width:600px;margin:auto;><code>conda install-c conda-forge jupyterlab</code></pre>
           
           <p>In the new version of jupyter lab 3.x the <code>tab</code> completion does not work, 
            and we need to downgrade the <code>jedi</code> library to 0.17.2. So, actually, we install:</p>
            <pre style=max-width:600px;margin:auto;>
                <code>conda install -c conda-forge jupyterlab</code>
                <code>conda install -c conda-forge jedi=0.17.2</code>
            </pre>
        
            <p>Additionally, I have created a <code>requiremets.txt</code> to install some extra frameworks and libraries such as: 
                <ol>
                    <li>Matplotlib</li>
                    <li>OpenCV</li>
                    <li>Scikit-learn</li>
                    <li>Pillow, etc</li>
                </ol>
                with the following command: <code>pip install -r requiremets.txt</code>
            </p>
        
        <h2>Register the environment</h2>
        <p>It is also a good practice to register the ipykernel for the given environment. 
            After activating the conda environment, in this case, is <code>conda
            activate tf24-cuda11</code> we use the following command to 
            <a href="https://ipython.readthedocs.io/en/stable/install/kernel_install.html">link</a>
            the kernel with the environment using <code>ipykernel install --user --name
            myenv --display-name "my_environment_name"</code>:</p>
             
            <pre style=max-width:600px;margin:auto;>
<code>ipykernel install --user --name tf24-cuda11 --display-name "Python 3.8.5 (tf24-cuda11)"</code></pre>
Test the environment

        <h2>Test the environment</h2>
        <p>After completing the above steps we can test the installation.</p>
        <pre style=max-width:600px;margin:auto;>
<code>
    import sys

    import tensorflow.keras
    import pandas as pd
    import sklearn as sk
    import tensorflow as tf
    
    print(f"Tensor Flow Version: {tf.__version__}")
    print(f"Keras Version: {tensorflow.keras.__version__}")
    print()
    print(f"Python {sys.version}")
    print(f"Pandas {pd.__version__}")
    print(f"Scikit-Learn {sk.__version__}")
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(logical_gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
    
    Tensor Flow Version: 2.4.1
    Keras Version: 2.4.0
    
    Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
    [GCC 7.3.0]
    Pandas 1.2.1
    Scikit-Learn 0.24.1
    1 Physical GPUs, 1 Logical GPUs
    Num GPUs Available:  1</code>
        </pre>

        <h2>Managing the Environment</h2>
        <p>So far we have created the environment for TensorFlow 2.4 with the appropriate 
            CUDA and cuDNN drivers installed through conda repos. Also, to make our life easier, 
            the different libraries are installed using the <code>requirements.txt</code>
            file. Now, it's a good opportunity to export the environment by creating a
            <code>.yml</code> file using the command:</p>

            <pre style=max-width:600px;margin:auto;>
                <code>conda env export > tf24-cuda11.yml</code>
            </pre>
            <p>Finally, with this way, of using Anaconda environments to install CUDA drivers, 
                we can install multiple versions of TensorFlow as well as other ML/DL frameworks, 
                without mess the systems libraries. Also, another way is to use Docker containers, 
                more details can be found:
                <ul>
                    <li><a href="https://www.tensorflow.org/install">In TensorFlow’s installation
                        guide</a></li>
                    <li><a href="https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77">In
                        this Medium article</a></li>
                    <li><a href="https://towardsdatascience.com/managing-cuda-dependencies-with-conda-89c5d817e7e1">Managing
                        CUDA dependencies with Conda</a></li>
                        
                    </ul>
                </p>
]]></description>
</item>

<item>
<title>ncl-naval</title>
<guid>https://ipolymenis.xyz/2022.html#nclnaval</guid>
<pubDate>Wed, 13 Apr 2022 16:23:23 +0100</pubDate>
<description><![CDATA[
<h2>Why I chose to study MSc Naval Architecture at Newcastle</h2>
            <h3>Course Content</h3>
            <p>I chose to study MSc in Naval Architecture at Newcastle University because I was particularly interested in the course's core modules.
                Having worked on ships as a seagoing engineer for more than ten years, I was keen on learning more about the principles that play a vital 
                role in the ship's operation. In addition, from my earlier studies, I was really enthusiastic about Fluid dynamics, particularly with ship 
                hydrodynamics, about the enormous ship structures and their performance at sea, and the operation and the logistics required to operate 
                and move ships across the world.
            </p>
            <p>          
                The course offers a straightforward though a challenging combination of all the above, some of the course's core modules are:
                <ul>
                    <li>Advanced Hydrodynamics</li>
                    <li>Advanced Marine Structures</li>
                    <li>Advanced Naval Architecture</li>
                    <li>Ship Performance at Sea</li>
                </ul>
            </p>


            <h3>Ranking and Reputation</h3>
                <p>The marine sector plays an essential role in the global economy and trade, but the field itself can be described as a close environment; and when it comes to reputation, it is essential to choose wisely where will you go to study. There are only a handful of choices in the UK or globally when it comes to Marine Technology studies.
                Newcastle University stands out because of its rich and long history and tradition in both Marine Engineering and Naval Architecture, and its reputation is well known among shipping companies.
                Marine Technology at Newcastle is also double accredited by the Royal Institution of Naval Architects (RINA) and the Institute of Marine Engineering, Science and Technology (IMarEST), which opens more opportunities!</p>

            <h3>Facilities and Academics</h3>
                <p>Let's start with the people who constitute the department. The academic staff are excellent, and with great knowledge, they are ready to help whenever needed.
                They are actively involved in lecturing, in research and with the industry. I found this extraordinary because there is the potential to expand our contacts from academia to industry. But the most important is that I feel like being a part of a larger family!
                Now regarding the facilities, Newcastle University is one of the most well-equipped universities, with the most important being the newly reconstructed Emmerson Cavitation tunnel in Blyth, the school's research vessel (The Princess Royal) and last but not least, the Towing tank and the Wind, Wave and Current tank (WWCT) in the campus.</p>
            <h3>The campus and the city</h3>
                <p>Newcastle upon Tyne!!! The name speaks by itself. By far one of the most beautiful cities in the world.
                Having sailed around the globe as a seafarer, I have visited many cities; some are huge and ugly, others are beautiful with rude people, others are cold and not interesting, but no city can be compared with Newcastle.
                The city of Newcastle has a pleasing balance between a large and small city and offers a range of exquisite places such as the quayside and the Jesmond Dene.
                It has a mixture of warm and welcoming local people (the Geordies) and people from all around the world, making the city one the most attractive.
                The campus itself is a remarkable place in the city's heart, with the most notable buildings being the Armstrong building (where the Marine department is located) and the Old Library.</p>
]]></description>
</item>


<item>
<title>ml-history</title>
<guid>https://ipolymenis.xyz/2022.html#mlhistory</guid>
<pubDate>Wed, 13 Apr 2022 16:20:47 +0100</pubDate>
<description><![CDATA[
<h2>Machine Learning a brief history</h2>
            <p style="text-align:center">
                <img style="max-height:30em;max-width:30em;text-align: center" src=../images/istockphoto-1328881695-612x612.jpg><a href="../images/istockphoto-1328881695-612x612.jpg"></a>
            </p>

            <div class="center">
                <p class="cryptoinfo">
                  <strong>  
                  A brief history of Machine Learning based on my work for Autonomous Underwater Intervention using Intelligent Computer Vision methods.  
                </strong>
                </p>
            </div>

            <p>
                The concept of Machine Learning originated in the 1950s when the Computer Science pioneer Alan Turing proposed the question “Can machines think?” in the paper “Computing Machinery and Intelligence” (Turing, 1950) and introduced the famous “Turing test” which explains how an intelligent machine should perform and other concepts that give life to modern Artificial Intelligence (AI) and Machine Learning (ML). The general concept that Turing proposes is how a general-purpose computer can learn to perform a specific task on its own. In classical computer programming, the human gives instructions (the program) and data to be processed to the computer and the output is the answer to the problem. In contrast, with machine learning, humans give the data and the answers to the computer and the outputs are the rules that the computer developed during the process, and these rules can be further applied to new data to generate novel answers.
            </p>
            <p>
                In the early days, Machine Learning was connected with Artificial Intelligence and tried to develop systems that were equipped with intelligence and could be adopted in complex environments to change computer behaviour accordingly (Langley, 2011). Thus, it can be said that early machine learning models focused on acquiring knowledge and understanding the surrounding environment, and only later focused on developing complex mathematical and statistical models (Michalski et al., 1983).
            </p>
            <p>
                Machine Learning, in the present, is a rapidly growing field and is used in many different areas and disciplines, among them are various industries such as the oil/gas industry and shipping industry, the financial sector, the healthcare system, autonomous cars and robots. One of the most important machine learning applications is image recognition and classification since the present world has abundant digital images. Other machine learning applications are found in day-to-day activities such as web searches, smartphone speech recognition systems, and camera face detection modes. The research around the Machine Learning field and generally in Artificial Intelligence is a strong and rapidly growing that has already developed self-driving car prototypes, autonomous Unmanned Aerial Vehicles (UAVs), and Autonomous Underwater Vehicles (AUVs) which are capable of self-navigating without human intervention.
            </p>
            <p>
                The growth of machine learning was mainly possible during the last decade because of the unprecedented expansion of computing power and the abundance of available data (Big Data), as well as the development of more sophisticated and efficient neural network algorithms which allowed the explosion of machine learning applications. Artificial Neural Networks were proposed for the first time in 1943 by McCulloch & Pitts, McCulloch & Pitts (1943) who introduced simple artificial neurons that were designed as electric circuits based on human biological neurons.
            </p>
            <p>
                In 1986 Rosenberg and Terrence (1986) introduce a revolutionary, for the time, network which was able to convert English text to speech, and could learn by itself to pronounce more than 20,000 words (Sejnowski & Rosenberg, 1986). Another significant breakthrough in the science of neural networks was the studies of Rumelhart ad Hinton (Rumelhart et al., 1986; Hinton, 1990) where they proposed a novel learning method using back-propagation in neuron networks. The back-propagation method repeatedly adjusts the weights between the neuron connections of the network to minimise the output error. In the same period (LeCun et al., 1990) used networks with back-propagation for handwritten digit recognition. The main objective of that research was to demonstrate that neural networks can be used for image recognition problems without complex data reprocessing. The handwritten digit recognition back-propagation algorithm was later used for the development of a multilayer neural network and the development of the MNIST Dataset for handwritten digit recognition (LeCun et al., 1998). Specifically designed Convolutional Neural Networks (CNNs) deal with the variability of the inputs which are images of handwritten digits.
            </p>
            <p>
                However, it was not until the mid of 2000s that the first significant breakthrough in machine learning and the use of deep learning networks occurred when Hinton and Salakhutdinov (2006) proposed a new way of training a deep neural network and Deep Learning was once more the focus. The work of (Hinton & Salakhutdinov, 2006) suggests that the training of deep models could be done one layer at a time, and they observed that the performance was particularly good with even just three hidden layers. This, deep learning model, was tested on handwritten digit images and gave much higher success in digit classification than any previous algorithms. The evolution of Machine Learning and Deep Learning was continued over the following years with many significant contributions in different areas of the field such as the use of unsupervised learning techniques for the development of an exceptional model which corrects any corruption of the input data as Vincent et al. (2008) presented.
            </p>
            <p>
                The developments in machine learning as well as in deep learning continued the over the following years in areas of image recognition, face recognition, speech recognition, natural language processing, real-time translation and even the first intelligent system, with Google’s Brain project of AlphaGo, which beat a human professional player in the game of Go without human intervention, in 2017 (Silver et al., 2017). Furthermore, in the image classification competition organised by ImageNet, the ILSVRC (ImageNet Large Scale Visual Recognition Challenge), the top five image classification error has been decreasing since its inception in 2009 (Jia Deng et al., 2009) and reached an astonishing 2.3%4 which is well below the human performance of around 5% error rate (Hu et al., 2018).

            </p>
            <p>
                The evolution of Machine Learning shows that, although it started as a branch of the AI, today it has evolved to be a completely different field of computer science focused more on mathematical and statistical models and theories, with the aim to train the machines to “learn” from data rather than acquiring a more abstract understanding of their environment as AI does. Therefore, with the boom of data as well as with the explosion in capabilities of computer hardware, Machine Learning models have become more efficient in specific tasks, such as image recognition, and consequently, this has made possible another, long-forgotten field, that of Deep Learning.
            </p>

            <p>
                <h2>References</h2>
                <ul>
                    <li>Hinton, G.E. (1990) Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence. 46 (1–2), 47–75.</li>
                    <li>Hinton, G.E., Osindero, S. & Teh, Y.W. (2006) A fast learning algorithm for deep belief nets. Neural Computation. 18 (7), 1527–1554.</li>
                    <li>Hinton, G.E. & Salakhutdinov, R.R. (2006) Reducing the dimensionality of data with neural networks. Science. 313 (5786), 504–507.</li>
                    <li>Hu, J., Shen, L. & Sun, G. (2018) ’Squeeze-and-Excitation Networks’, in Proceedings of the ieee computer society conference on computer vision and pattern recognition. [Online]. 2018 pp. 71327141.</li>
                    <li>Langley, P. (2011) The changing science of machine learning. Machine Learning. 82 (3), 275–279.</li>
                    <li>LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E. & Jackel, L.D. (1990) ’Handwritten Digit Recognition with a Back-Propagation Network’, in Advances in neural information processing systems. [Online]. 1990 pp. 396 — –404.</li>
                    <li>LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. (1998) Gradient-based learning applied to document recognition. Proceedings of the IEEE. 86 (11), 2278–2323.</li>
                    <li>LeCun, Y., Jackel, L.D., Bottou, L., Cortes, C., Denker, J.S., Drucker, H., Guyon, I., Muller, U.a., Sackinger, E., Simard, P. & Vapnik, V. (1995) ’Learning algorithms for classification: A comparison on handwritten digit recognition’, in Neural networks: The statistical mechanics perspective. [Online]. World Scientific. pp. 261–276.</li>
                    <li>McCulloch, W.S. & Pitts, W. (1943) A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 5 (4), 115–133.</li>
                    <li>Rumelhart, D.E., Hinton, G.E. & Williams, R.J. (1986) Learning representations by back-propagating errors. Nature. 323 (6088), 533–536.</li>
                    <li>Sejnowski, T. & Rosenberg, C. (1986) NETtalk: A parallel network that learns to read aloud. The Johns Hopkins University Electrical Engineering and Computer Science Technical Report. 01663–672.</li>
                    <li>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., Van Den Driessche, G., Graepel, T. & Hassabis, D. (2017) Mastering the game of Go without human knowledge. Nature. 550 (7676), 354–359.</li>
                    <li>Turing, A.M. (1950) ’Computing machinery and intelligence’, in Machine intelligence: Perspectives on the computational model. [Online]. Taylor; Francis. pp. 1–28.</li>
                    <li>Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.A. (2008) ’Extracting and composing robust features with denoising autoencoders’, in Proceedings of the 25th international conference on machine learning. [Online]. 2008 pp. 1096–1103.</li>
                    </ul>
            </p>
]]></description>
</item>

</channel>
</rss>
